{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "colab": {
   "name": "awkward-GAN-with-no-warning.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLFTIlb2dRz7",
    "colab_type": "text"
   },
   "source": [
    "DeGAN neural network with generative extra training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV9K8a6ldRz8",
    "colab_type": "text"
   },
   "source": [
    "#### Loading libs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y5e-1H0GdRz9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# for data input and output:\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# for deep learning: \n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, Lambda\n",
    "from keras.layers import BatchNormalization, Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Reshape # new! \n",
    "from keras.layers import Conv2DTranspose, UpSampling2D # new! \n",
    "from keras.optimizers import RMSprop # new!\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "# for plotting: \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### This code uses once per colab session to download *pydot* and *graphviz* libs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip3 install pydotplus\n",
    "!pip3 install pydot\n",
    "!apt-get -qq install -y graphviz && pip3 install -q pydot\n",
    "!pip3 install pydot-ng\n",
    "!pip3 install graphviz\n",
    "!apt-get install python-pydot python-pydot-ng graphviz\n",
    "import pydot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp3CNu64dR0D",
    "colab_type": "text"
   },
   "source": [
    "#### IN COLAB Load data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jFR7C2g1iijS",
    "colab_type": "code",
    "outputId": "bf51c99e-b4da-4387-d58d-ddea021a6e26",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "input_images = \"/content/drive/My Drive/full_numpy_bitmap_castle.npy\"\n",
    "data = np.load(input_images)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LOCALLY Loading data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "input_images = \"D:/full_numpy_bitmap_castle.npy\"\n",
    "data = np.load(input_images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Some input data tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GqWQZUYIdR0N",
    "colab_type": "code",
    "outputId": "751f1c3c-a8e7-45a4-f4cb-6b4a72ddf568",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "data.shape #output amount of pics and dimensions"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(122534, 784)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "53RjYD8ddR0Q",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "data[3117] #output pic 3117"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,  13,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 106, 179,  95,\n        16,  33, 208,  50, 199, 229, 250, 255,  98,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 238,\n       244, 255, 232,  87, 255, 119, 255, 174, 139, 255, 104,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n       105, 255,  50, 131, 255, 123, 255, 125, 255,  66,  41, 255,  82,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 164,\n       251, 255, 255, 194,   0, 116, 255, 212, 255, 135, 255,  65,  61,\n       255,  58,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0, 230, 194, 119, 115,  36,   0,  40, 183, 249, 255, 244, 246,\n        19,  11, 111,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   8, 254, 116,   0,   0,   0,   0,   0,   0,  13, 120,\n       151,  93,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,  38, 255,  85,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,  51,  38,   5,   0,  76, 130, 255,  54,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  58,\n       224, 237, 115, 255, 255, 205,  33, 254, 255, 255,  23,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0, 123, 255, 239, 249, 254, 145, 255, 202, 253, 191, 244,   1,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0, 123, 254,  75, 165, 153,  22, 226, 209, 128,  39,\n        85,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0, 123, 254,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0, 123, 254,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0, 123, 254,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 123, 254,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n       123, 254,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0, 123, 254,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0, 123, 254,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0, 123, 254,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0, 100, 232,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   7,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0], dtype=uint8)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Normalizing data and reshape into square pic\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oE0q--xGdR0U",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "data = data/255\n",
    "data = np.reshape(data,(data.shape[0],28,28,1)) # fourth dimension is color\n",
    "img_w,img_h = data.shape[1:3]"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Some reshaped data tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.shape #output amount of pics and dimensions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[3117] #output pic 3117"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JPN-I8eZdR0f",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "plt.imshow(data[3117,:,:,0], cmap='Greys') #output bitmap as a picture"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x157c0be8348>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qwWMTAadR0k",
    "colab_type": "text"
   },
   "source": [
    "#### Create network of discriminator function\n",
    "*depth* - base neurons amount in hidden layer\n",
    "\n",
    "*p* - dropout coefficient\n",
    "\n",
    "return discriminator network model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3byUIAuldR0m",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def build_discriminator(depth=64, p=0.4):\n",
    "\n",
    "    # input layer\n",
    "    image = Input((img_w,img_h,1))\n",
    "    \n",
    "    # Hide layers\n",
    "    conv1 = Conv2D(depth*1, 5, strides=2, \n",
    "                   padding='same', activation='relu')(image)\n",
    "    conv1 = Dropout(p)(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(depth*2, 5, strides=2, \n",
    "                   padding='same', activation='relu')(conv1)\n",
    "    conv2 = Dropout(p)(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(depth*4, 5, strides=2, \n",
    "                   padding='same', activation='relu')(conv2)\n",
    "    conv3 = Dropout(p)(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(depth*8, 5, strides=1, \n",
    "                   padding='same', activation='relu')(conv3)\n",
    "    conv4 = Flatten()(Dropout(p)(conv4))\n",
    "    \n",
    "    # Output layer\n",
    "    prediction = Dense(1, activation='sigmoid', name='prediction')(conv4)\n",
    "    \n",
    "    # Model construction\n",
    "    model = Model(inputs=image, outputs=prediction)\n",
    "\n",
    "    # printing summary info about discriminator\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='discriminator_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    return model"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compiling discriminator model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8MPJpGdedR0q",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=RMSprop(lr=0.0008,\n",
    "                                        decay=6e-8,\n",
    "                                        clipvalue=1.0),\n",
    "                      metrics=['binary_accuracy'])"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBxm4p-0dR04",
    "colab_type": "text"
   },
   "source": [
    "#### Build an untrainable discriminator\n",
    "Full copy of trainable discriminator, for avoiding unfixed error in Keras."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3cp6avzmdR05",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Build second discriminator:\n",
    "discriminator_ = build_discriminator()\n",
    "\n",
    "# Deny training of this discriminator\n",
    "discriminator_.trainable = False\n",
    "\n",
    "# Copying actual parameters into frozen discriminator\n",
    "def copy_weights(source, target):\n",
    "    for i, layer in enumerate(source.layers):\n",
    "        target.layers[i].set_weights(source.layers[i].get_weights())"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKNyKTK4dR0_",
    "colab_type": "text"
   },
   "source": [
    "#### Create generator network"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jRPTap90dR0_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "z_dimensions = 32 #input noise signal dimensions amount"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define generator network function\n",
    "*latent_dim* - input dimensions\n",
    "\n",
    "*depth* - base neurons amount in hidden layer\n",
    "\n",
    "*p* - dropout coefficient\n",
    "\n",
    "return generator network model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7vW3fO5IdR1C",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def build_generator(latent_dim=z_dimensions, \n",
    "                    depth=64, p=0.4):\n",
    "    \n",
    "    # Inputs\n",
    "    noise = Input((latent_dim,))\n",
    "    \n",
    "    # Input preparation and reshaping\n",
    "    dense1 = Dense(7*7*depth)(noise)\n",
    "    dense1 = BatchNormalization(momentum=0.9)(dense1) # default momentum for moving average is 0.99\n",
    "    dense1 = Activation(activation='relu')(dense1)\n",
    "    dense1 = Reshape((7,7,depth))(dense1)\n",
    "    dense1 = Dropout(p)(dense1)\n",
    "    \n",
    "    # Deconvolutional layers with batch normalisation\n",
    "    conv1 = UpSampling2D()(dense1)\n",
    "    conv1 = Conv2DTranspose(int(depth/2), \n",
    "                            kernel_size=5, padding='same', \n",
    "                            activation=None,)(conv1)\n",
    "    conv1 = BatchNormalization(momentum=0.9)(conv1)\n",
    "    conv1 = Activation(activation='relu')(conv1)\n",
    "    \n",
    "    conv2 = UpSampling2D()(conv1)\n",
    "    conv2 = Conv2DTranspose(int(depth/4), \n",
    "                            kernel_size=5, padding='same', \n",
    "                            activation=None,)(conv2)\n",
    "    conv2 = BatchNormalization(momentum=0.9)(conv2)\n",
    "    conv2 = Activation(activation='relu')(conv2)\n",
    "    \n",
    "    conv3 = Conv2DTranspose(int(depth/8), \n",
    "                            kernel_size=5, padding='same', \n",
    "                            activation=None,)(conv2)\n",
    "    conv3 = BatchNormalization(momentum=0.9)(conv3)\n",
    "    conv3 = Activation(activation='relu')(conv3)\n",
    "\n",
    "    # Output\n",
    "    image = Conv2D(1, kernel_size=5, padding='same', \n",
    "                   activation='sigmoid')(conv3)\n",
    "\n",
    "    # Model construction\n",
    "    model = Model(inputs=noise, outputs=image)\n",
    "\n",
    "    # printing summary info about generator\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='generator_model_plot1.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    return model"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compiling generator model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0qxXj2pndR1H",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy',\n",
    "                          optimizer=RMSprop(lr=0.0004,\n",
    "                                            decay=3e-8,\n",
    "                                            clipvalue=1.0),\n",
    "                          metrics=['accuracy'])"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s0HG3npdR1W",
    "colab_type": "text"
   },
   "source": [
    "#### Building adversarial network function\n",
    "*no parameters*\n",
    "\n",
    "return adversarial network model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ePz74nFsdR1X",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def build_adversarial():\n",
    "          \n",
    "    # Input\n",
    "    z = Input(shape=(z_dimensions,))\n",
    "    \n",
    "    # Generate input noise vector\n",
    "    img = generator(z)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = discriminator_(img)\n",
    "       \n",
    "    # Build model\n",
    "    model = Model(inputs=z, outputs=pred)\n",
    "\n",
    "    # printing summary info about whole adversarial network\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='adv_model_plot1.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    return model"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Build adversarial network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P0UpuHXUdR1b",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "adversarial_model = build_adversarial()\n",
    "adversarial_model.compile(loss='binary_crossentropy',\n",
    "                          optimizer=RMSprop(lr=0.0004, \n",
    "                                            decay=3e-8, \n",
    "                                            clipvalue=1.0), \n",
    "                          metrics=['accuracy'])"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKc54rnvdR1l",
    "colab_type": "text"
   },
   "source": [
    "#### Training function\n",
    "*epoch* - number of epoch\n",
    "\n",
    "*batch* - number of pics into one training batch\n",
    "\n",
    "*z_dim* - number of input noise signal dimensions\n",
    "\n",
    "return adversarial network metrics, discriminator metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P85sH2ZXdR1m",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def train(epochs=2000, batch=128, z_dim=z_dimensions):\n",
    "    \n",
    "    d_metrics = []\n",
    "    a_metrics = []\n",
    "    \n",
    "    running_d_loss = 0\n",
    "    running_d_acc = 0\n",
    "    running_a_loss = 0\n",
    "    running_a_acc = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # train discriminator\n",
    "        # ________________________________________\n",
    "        # real pics with randomisation:\n",
    "        real_imgs = np.reshape(\n",
    "            data[np.random.choice(data.shape[0],\n",
    "                                  batch,\n",
    "                                  replace=False)],\n",
    "            (batch,28,28,1))\n",
    "        \n",
    "        # generate fake pics:\n",
    "        fake_imgs = generator.predict(\n",
    "            np.random.uniform(-1.0, 1.0, \n",
    "                              size=[batch, z_dim]))\n",
    "        \n",
    "        # concatenate real and fake pics:\n",
    "        x = np.concatenate((real_imgs,fake_imgs))\n",
    "        \n",
    "        # assign y labels for discriminator:\n",
    "        y = np.ones([2*batch,1]) - np.random.uniform(0,0.1,[2*batch,1])\n",
    "        y[batch:,:] = 0\n",
    "        y[batch:,:] += np.random.uniform(0,0.1,[batch,1])\n",
    "        \n",
    "        # make training\n",
    "        d_metrics.append(\n",
    "            discriminator.train_on_batch(x,y)\n",
    "        )\n",
    "        running_d_loss += d_metrics[-1][0]\n",
    "        running_d_acc += d_metrics[-1][1]\n",
    "        \n",
    "\n",
    "        # train adversarial network\n",
    "        # ________________________________________\n",
    "        \n",
    "        # noise input\n",
    "        noise = np.random.uniform(-1.0, 1.0, \n",
    "                                  size=[batch, z_dim])\n",
    "        y = np.ones([batch,1])\n",
    "        \n",
    "        # Copy weights to second discriminator\n",
    "        copy_weights(discriminator, discriminator_)\n",
    "        \n",
    "        # make train\n",
    "        a_metrics.append(\n",
    "            adversarial_model.train_on_batch(noise,y)\n",
    "        ) \n",
    "        running_a_loss += a_metrics[-1][0]\n",
    "        running_a_acc += a_metrics[-1][1]\n",
    "\n",
    "        #     Results     #\n",
    "        # ________________________________________\n",
    "        \n",
    "        # periodically print progress and fake images:\n",
    "        if (i+1)%20 == 0:\n",
    "\n",
    "            print('Epoch #{}'.format(i))\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % \\\n",
    "            (i, running_d_loss/i, running_d_acc/i)\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % \\\n",
    "            (log_mesg, running_a_loss/i, running_a_acc/i)\n",
    "            print(log_mesg)\n",
    "\n",
    "            noise = np.random.uniform(-1.0, 1.0, \n",
    "                                      size=[16, z_dim])\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            plt.figure(figsize=(5,5))\n",
    "\n",
    "            for k in range(gen_imgs.shape[0]):\n",
    "                plt.subplot(4, 4, k+1)\n",
    "                plt.imshow(gen_imgs[k, :, :, 0], \n",
    "                           cmap='gray')\n",
    "                plt.axis('off')\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    return a_metrics, d_metrics"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Start training procedure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1Vq5LFrjdR1r",
    "colab_type": "code",
    "outputId": "983b94d7-6c82-4fc3-9984-37da2da12f99",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "a_metrics_complete, d_metrics_complete = train()"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-28-84d30fd151dd>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0ma_metrics_complete\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0md_metrics_complete\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-27-c814c12b393c>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(epochs, batch, z_dim)\u001B[0m\n\u001B[0;32m     39\u001B[0m         \u001B[1;31m# do training\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     40\u001B[0m         d_metrics.append(\n\u001B[1;32m---> 41\u001B[1;33m             \u001B[0mdiscriminator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_on_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     42\u001B[0m         )\n\u001B[0;32m     43\u001B[0m         \u001B[0mrunning_d_loss\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0md_metrics\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\deGAN\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mtrain_on_batch\u001B[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001B[0m\n\u001B[0;32m   1512\u001B[0m             \u001B[0mins\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0my\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0msample_weights\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1513\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_train_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1514\u001B[1;33m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mins\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1515\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1516\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mreset_metrics\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\deGAN\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   3725\u001B[0m         \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmath_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3726\u001B[0m       \u001B[0mconverted_inputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3727\u001B[1;33m     \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_graph_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mconverted_inputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3728\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3729\u001B[0m     \u001B[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\deGAN\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1549\u001B[0m       \u001B[0mTypeError\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mFor\u001B[0m \u001B[0minvalid\u001B[0m \u001B[0mpositional\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mkeyword\u001B[0m \u001B[0margument\u001B[0m \u001B[0mcombinations\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1550\u001B[0m     \"\"\"\n\u001B[1;32m-> 1551\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1552\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1553\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcancellation_manager\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\deGAN\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, args, kwargs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1589\u001B[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001B[0;32m   1590\u001B[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001B[1;32m-> 1591\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_flat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcancellation_manager\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1592\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1593\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_filtered_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\deGAN\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1690\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1691\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[1;32m-> 1692\u001B[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[0;32m   1693\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[0;32m   1694\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\deGAN\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    543\u001B[0m               \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    544\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"executor_type\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mexecutor_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"config_proto\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 545\u001B[1;33m               ctx=ctx)\n\u001B[0m\u001B[0;32m    546\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    547\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[1;32m~\\miniconda3\\envs\\deGAN\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     59\u001B[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001B[0;32m     60\u001B[0m                                                \u001B[0mop_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 61\u001B[1;33m                                                num_outputs)\n\u001B[0m\u001B[0;32m     62\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Print Loss and Accuracy plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0wSGGpOAdR1v",
    "colab_type": "code",
    "outputId": "3b86f94c-390c-4511-a366-93d7953a215a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    }
   },
   "source": [
    "ax = pd.DataFrame(\n",
    "    {\n",
    "        'Adversarial': [metric[0] for metric in a_metrics_complete],\n",
    "        'Discriminator': [metric[0] for metric in d_metrics_complete],\n",
    "    }\n",
    ").plot(title='Training Loss', logy=True)\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z3YCqNbhdR1x",
    "colab_type": "code",
    "outputId": "ea9fe453-fd00-4a41-d644-4daa7b1b3ab2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    }
   },
   "source": [
    "ax = pd.DataFrame(\n",
    "    {\n",
    "        'Adversarial': [metric[1] for metric in a_metrics_complete],\n",
    "        'Discriminator': [metric[1] for metric in d_metrics_complete],\n",
    "    }\n",
    ").plot(title='Training Accuracy')\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Accuracy\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}